{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMsHOuokIYLsytulmVyD9F8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunAravind2001/Named-Entity-Recognition-Model/blob/main/NER_MOdel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUy7KtU_8WPJ",
        "outputId": "397fc942-0924-4c5b-9cab-94ae671928ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training import Example\n",
        "from spacy.util import minibatch\n"
      ],
      "metadata": {
        "id": "vF-NDj9T8Y5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('Labelled_dataset.xlsx')\n"
      ],
      "metadata": {
        "id": "sRKKaWyk8jYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Create a DocBin object to store training examples\n",
        "doc_bin = DocBin()\n",
        "\n",
        "# Iterate over the rows of the DataFrame to create training examples\n",
        "for _, row in df.iterrows():\n",
        "    text = row['Article_Text']\n",
        "    spans = []\n",
        "\n",
        "    # Extract entity information from other columns\n",
        "    for col in df.columns:\n",
        "        if col not in ['Article_No', 'Article_Text']:\n",
        "            start = text.find(str(row[col]))\n",
        "            end = start + len(str(row[col]))\n",
        "            spans.append(Span(doc=nlp.make_doc(text), start=start, end=end, label=col))\n",
        "\n",
        "    doc = nlp.make_doc(text)\n",
        "    for span in spans:\n",
        "        doc.spans[span.label] = doc.spans.get(span.label, []) + [span]\n",
        "    example = Example.from_dict(doc, {})\n",
        "    doc_bin.add(doc)\n",
        "\n",
        "# Save the training data to disk\n",
        "doc_bin.to_disk(\"training_data.spacy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "cJWOWMAe8niO",
        "outputId": "3e38d900-5ba1-4507-b4f5-1b37d9c794e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Span' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5407d3ce1ffc>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mspans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Span' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin, Span\n",
        "from spacy.training import Example\n",
        "import pandas as pd\n",
        "\n",
        "# Load a pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # You can also use larger models\n",
        "\n",
        "# Create a DocBin object to store training examples\n",
        "doc_bin = DocBin()\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('labelled_dataset.xlsx')\n",
        "\n",
        "# Iterate over the rows of the DataFrame to create training examples\n",
        "for _, row in df.iterrows():\n",
        "    text = row['Article_Text']\n",
        "    doc = nlp.make_doc(text)  # Create the Doc object once\n",
        "    spans = []  # List to store spans\n",
        "\n",
        "    # Extract entity information from other columns\n",
        "    for col in df.columns:\n",
        "        if col not in ['Article_No', 'Article_Text']:\n",
        "            value = str(row[col])\n",
        "            start = text.find(value)  # Find the start index\n",
        "            if start != -1:  # Ensure the value was found\n",
        "                end = start + len(value)  # Calculate the end index\n",
        "                if end <= len(doc):  # Check if the end index is valid\n",
        "                    spans.append(Span(doc, start, end, label=str(col)))  # Create the Span\n",
        "\n",
        "    # Create a list of entity tuples for the Example\n",
        "    entity_tuples = [(span.start, span.end, span.label_) for span in spans]\n",
        "\n",
        "    # Check alignment with spaCy's built-in function\n",
        "    # If misalignment warnings are still shown, it might be necessary to ensure that spans\n",
        "    # correspond correctly with actual text. This part can be skipped if alignment is confirmed.\n",
        "    try:\n",
        "        example = Example.from_dict(doc, {\"entities\": entity_tuples})\n",
        "        doc_bin.add(example.reference)  # Add the reference doc to the DocBin\n",
        "    except Exception as e:\n",
        "        print(f\"Error with text: {text}, {e}\")\n",
        "\n",
        "# Save the training data to disk\n",
        "doc_bin.to_disk(\"training_data.spacy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9w8urTc9LCK",
        "outputId": "27380edf-c830-4c59-b675-4352b42e9d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Garmin(NYSE: GRMN) acquired Lumishore, a Swansea, ...\" with entities \"[(28, 37, 'Company_Receiving_Investment'), (0, 6, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6jK1mXrADay",
        "outputId": "e486937a-363f-4b72-f8d1-6e08eb8b63fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Article_No', 'Company_Receiving_Investment', 'Investment_Amount',\n",
            "       'Investing_Companies', 'Round_of_Investment', 'Location_of_the_Company',\n",
            "       'Date', 'CEO_Name', 'Article_Text'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin, Span\n",
        "from spacy.training import Example\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load a pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # You can also use larger models\n",
        "\n",
        "# Create a DocBin object to store training examples\n",
        "doc_bin = DocBin()\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('labelled_dataset.xlsx')\n",
        "\n",
        "# Create a blank NER pipeline if it doesn't exist\n",
        "if \"ner\" not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe(\"ner\")\n",
        "    nlp.add_pipe(ner, last=True)\n",
        "else:\n",
        "    ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "# Add your new labels to the NER model from the DataFrame columns\n",
        "for col in df.columns:\n",
        "    if col not in ['Article_No', 'Article_Text']:\n",
        "        ner.add_label(col)  # Add labels from relevant columns\n",
        "\n",
        "# Prepare training data\n",
        "training_data = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    text = row['Article_Text']\n",
        "    entities = []\n",
        "\n",
        "    # Create the entities from the other columns\n",
        "    for col in df.columns:\n",
        "        if col not in ['Article_No', 'Article_Text']:\n",
        "            value = str(row[col])\n",
        "            start = text.find(value)\n",
        "            if start != -1:\n",
        "                end = start + len(value)\n",
        "                entities.append((start, end, col))  # Create a tuple for the entity\n",
        "\n",
        "    training_data.append((text, {\"entities\": entities}))  # Append the text and entities to the training data\n",
        "\n",
        "# Train the model\n",
        "optimizer = nlp.resume_training()\n",
        "for i in range(10):  # Number of iterations\n",
        "    losses = {}\n",
        "    # Shuffle the training data\n",
        "    random.shuffle(training_data)\n",
        "    # Create minibatches\n",
        "    for batch in spacy.util.minibatch(training_data, size=8):  # Size can be adjusted\n",
        "        for text, annotations in batch:\n",
        "            example = Example.from_dict(nlp.make_doc(text), annotations)\n",
        "            nlp.update([example], drop=0.5, losses=losses)\n",
        "    print(f\"Iteration {i} - Losses: {losses}\")\n",
        "\n",
        "# Save the trained model\n",
        "nlp.to_disk(\"trained_ner_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9MhGdGh-8a9",
        "outputId": "e3535cc4-c01d-4e20-c523-c3f0161e941b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Garmin(NYSE: GRMN) acquired Lumishore, a Swansea, ...\" with entities \"[(28, 37, 'Company_Receiving_Investment'), (0, 6, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Hong Kong, Singapore, London, and Dubai, Global, O...\" with entities \"[(81, 87, 'Company_Receiving_Investment'), (2066, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Alvara Protocol, a London, UK-based provider of a ...\" with entities \"[(0, 15, 'Company_Receiving_Investment'), (130, 13...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"London, United Kingdom, October 22nd, 2024, Chainw...\" with entities \"[(55, 62, 'Company_Receiving_Investment'), (965, 9...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 306.31614880721605}\n",
            "Iteration 1 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 94.53651996785375}\n",
            "Iteration 2 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 80.62561745109119}\n",
            "Iteration 3 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 72.04632275520727}\n",
            "Iteration 4 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 69.91087591655074}\n",
            "Iteration 5 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 65.73261862847896}\n",
            "Iteration 6 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 363.8939645022619}\n",
            "Iteration 7 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 105.08298369521728}\n",
            "Iteration 8 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 178.78157152307375}\n",
            "Iteration 9 - Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 64.32698021825931}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "nlp_trained = spacy.load(\"trained_ner_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWUeP43S_51w",
        "outputId": "1e483353-b418-4507-d75e-536e43248f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained NER model\n",
        "nlp_trained = spacy.load(\"trained_ner_model\")\n",
        "\n",
        "# The test article\n",
        "test_article = \"\"\"\n",
        "With Nothing Underneath, a London, UK-based womenswear brand, raised £2.5M in funding. The round was led by Pembroke VCT alongside JamJar Investments. The company intends to use the funds to accelerate the expansion of its team and of its UK retail presence, grow its presence in overseas markets in the U.S. and Europe – specifically in the DACH region (Germany, Austria and Switzerland), and further develop new product lines. Founded in 2017 by Pip Durell, former Tatler editor and Vogue stylist, With Nothing Underneath provides shirts and other wardrobe staples with inspiration drawn from icons like Carolyn Bessette and Charlotte Rampling. Their sourcing and practices are recognised by a B Corp certification. The business has already generated an international customer base and received organic endorsements from the likes of Dolly Alderton, Cara Delevingne and Florence Pugh. It has also collaborated with well-known brand, Free People, and has a limited-edition partnership with the Financial Times. FinSMEs 21/10/2024\n",
        "\"\"\"\n",
        "\n",
        "# Process the article through the trained model\n",
        "doc = nlp_trained(test_article)\n",
        "\n",
        "# Function to extract information in a structured format\n",
        "def extract_information(doc):\n",
        "    extracted_info = {\n",
        "        \"Company_Receiving_Investment\": \"N/A\",\n",
        "        \"Investment_Amount\": \"N/A\",\n",
        "        \"Investing_Companies\": \"N/A\",\n",
        "        \"Round_of_Investment\": \"N/A\",\n",
        "        \"Location_of_the_Company\": \"N/A\",\n",
        "        \"Date\": \"N/A\",\n",
        "        \"CEO_Name\": \"N/A\"\n",
        "    }\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"Company_Receiving_Investment\":\n",
        "            extracted_info[\"Company_Receiving_Investment\"] = ent.text\n",
        "        elif ent.label_ == \"Investment_Amount\":\n",
        "            extracted_info[\"Investment_Amount\"] = ent.text\n",
        "        elif ent.label_ == \"Investing_Companies\":\n",
        "            extracted_info[\"Investing_Companies\"] = ent.text\n",
        "        elif ent.label_ == \"Round_of_Investment\":\n",
        "            extracted_info[\"Round_of_Investment\"] = ent.text\n",
        "        elif ent.label_ == \"Location_of_the_Company\":\n",
        "            extracted_info[\"Location_of_the_Company\"] = ent.text\n",
        "        elif ent.label_ == \"Date\":\n",
        "            extracted_info[\"Date\"] = ent.text\n",
        "        elif ent.label_ == \"CEO_Name\":\n",
        "            extracted_info[\"CEO_Name\"] = ent.text\n",
        "\n",
        "    return extracted_info\n",
        "\n",
        "# Extract information from the test article\n",
        "info = extract_information(doc)\n",
        "\n",
        "# Display the extracted information\n",
        "print(f\"Extracted Information: {info}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJQeuwqJATia",
        "outputId": "4cee3040-c4d2-4619-a46c-12486e8bf9b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Information: {'Company_Receiving_Investment': 'N/A', 'Investment_Amount': 'N/A', 'Investing_Companies': 'N/A', 'Round_of_Investment': 'N/A', 'Location_of_the_Company': 'London, UK', 'Date': 'N/A', 'CEO_Name': 'N/A'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import spacy\n",
        "from spacy.tokens import DocBin, Span\n",
        "from spacy.training import Example\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load a pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # You can also use larger models like \"en_core_web_md\"\n",
        "\n",
        "# Load your labeled dataset\n",
        "df = pd.read_excel('labelled_dataset.xlsx')\n",
        "\n",
        "# Create a DocBin object to store training examples\n",
        "doc_bin = DocBin()\n",
        "\n",
        "# Add custom labels to the NER model\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "for label in df.columns[1:]:  # Assuming the first column is 'Article_No'\n",
        "    ner.add_label(label)\n",
        "\n",
        "# Prepare training data\n",
        "for _, row in df.iterrows():\n",
        "    text = row['Article_Text']\n",
        "    doc = nlp.make_doc(text)  # Create the Doc object once\n",
        "    spans = []  # List to store spans\n",
        "\n",
        "    # Extract entity information from other columns\n",
        "    for col in df.columns[1:]:  # Skip 'Article_No' and focus on entity columns\n",
        "        value = str(row[col])\n",
        "        start = text.find(value)  # Find the start index\n",
        "        if start != -1:  # Ensure the value was found\n",
        "            end = start + len(value)  # Calculate the end index\n",
        "            if end <= len(doc):  # Check if the end index is valid\n",
        "                spans.append(Span(doc, start, end, label=col))  # Create the Span\n",
        "\n",
        "    # Create a list of entity tuples for the Example\n",
        "    entity_tuples = [(span.start, span.end, span.label_) for span in spans]\n",
        "\n",
        "    # Create an example and add it to the DocBin\n",
        "    example = Example.from_dict(doc, {\"entities\": entity_tuples})\n",
        "    doc_bin.add(example.reference)  # Add the reference doc to the DocBin\n",
        "\n",
        "# Save the training data to disk\n",
        "doc_bin.to_disk(\"training_data.spacy\")\n",
        "\n",
        "# Prepare for training\n",
        "nlp.resume_training()  # Start/resume training process\n",
        "random.seed(42)\n",
        "\n",
        "# Convert generator to list for shuffling\n",
        "docs = list(doc_bin.get_docs(nlp.vocab))\n",
        "random.shuffle(docs)  # Shuffle training data\n",
        "\n",
        "# Training Loop\n",
        "for i in range(10):  # Number of iterations (adjustable)\n",
        "    losses = {}\n",
        "    for doc in docs:  # Loop through each training example\n",
        "        example = Example.from_dict(doc, {\"entities\": doc.ents})\n",
        "        nlp.update([example], drop=0.5, losses=losses)\n",
        "    print(f\"Iteration {i + 1}: Losses: {losses}\")\n",
        "\n",
        "# Use the trained model on a new article\n",
        "test_article = \"\"\"\n",
        "With Nothing Underneath, a London, UK-based womenswear brand, raised £2.5M in funding. The round was led by Pembroke VCT alongside JamJar Investments. The company intends to use the funds to accelerate the expansion of its team and of its UK retail presence, grow its presence in overseas markets in the U.S. and Europe – specifically in the DACH region (Germany, Austria and Switzerland), and further develop new product lines. Founded in 2017 by Pip Durell, former Tatler editor and Vogue stylist, With Nothing Underneath provides shirts and other wardrobe staples with inspiration drawn from icons like Carolyn Bessette and Charlotte Rampling. Their sourcing and practices are recognised by a B Corp certification. The business has already generated an international customer base and received organic endorsements from the likes of Dolly Alderton, Cara Delevingne and Florence Pugh. It has also collaborated with well-known brand, Free People, and has a limited-edition partnership with the Financial Times. FinSMEs 21/10/2024\n",
        "\"\"\"\n",
        "\n",
        "# Process the test article\n",
        "doc = nlp(test_article)\n",
        "\n",
        "# Extract entities\n",
        "extracted_info = {\n",
        "    \"Company_Receiving_Investment\": \"N/A\",\n",
        "    \"Investment_Amount\": \"N/A\",\n",
        "    \"Investing_Companies\": \"N/A\",\n",
        "    \"Round_of_Investment\": \"N/A\",\n",
        "    \"Location_of_the_Company\": \"N/A\",\n",
        "    \"Date\": \"N/A\",\n",
        "    \"CEO_Name\": \"N/A\"\n",
        "}\n",
        "\n",
        "# Populate extracted_info with values from the doc.ents\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ == \"Company_Receiving_Investment\":\n",
        "        extracted_info[\"Company_Receiving_Investment\"] = ent.text\n",
        "    elif ent.label_ == \"Investment_Amount\":\n",
        "        extracted_info[\"Investment_Amount\"] = ent.text\n",
        "    elif ent.label_ == \"Investing_Companies\":\n",
        "        if extracted_info[\"Investing_Companies\"] == \"N/A\":\n",
        "            extracted_info[\"Investing_Companies\"] = ent.text\n",
        "        else:\n",
        "            extracted_info[\"Investing_Companies\"] += f\", {ent.text}\"\n",
        "    elif ent.label_ == \"Location_of_the_Company\":\n",
        "        extracted_info[\"Location_of_the_Company\"] = ent.text\n",
        "    elif ent.label_ == \"Date\":\n",
        "        extracted_info[\"Date\"] = ent.text\n",
        "    elif ent.label_ == \"CEO_Name\":\n",
        "        extracted_info[\"CEO_Name\"] = ent.text\n",
        "\n",
        "# Print the extracted information in the desired format\n",
        "print(\"Extracted Information:\", extracted_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO2rzFYcAo9D",
        "outputId": "b3416fe6-0e79-473f-ad82-78d15a76a633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Garmin(NYSE: GRMN) acquired Lumishore, a Swansea, ...\" with entities \"[(28, 37, 'Company_Receiving_Investment'), (0, 6, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 293.1393080661691}\n",
            "Iteration 2: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 117.97375602335808}\n",
            "Iteration 3: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 96.18403791534183}\n",
            "Iteration 4: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 85.75334678070358}\n",
            "Iteration 5: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 79.81872612897276}\n",
            "Iteration 6: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 76.4446043418578}\n",
            "Iteration 7: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 70.95285601470798}\n",
            "Iteration 8: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 68.55609506440729}\n",
            "Iteration 9: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 56.58014587031191}\n",
            "Iteration 10: Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 50.944618123728105}\n",
            "Extracted Information: {'Company_Receiving_Investment': 'N/A', 'Investment_Amount': 'N/A', 'Investing_Companies': 'N/A', 'Round_of_Investment': 'N/A', 'Location_of_the_Company': 'London, UK', 'Date': 'N/A', 'CEO_Name': 'N/A'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_entity_positions(text, entities):\n",
        "    \"\"\"\n",
        "    Calculate the start and end positions of each entity in the given text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The text to search.\n",
        "    entities (dict): A dictionary of entity names and their values.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary with the start and end positions for each entity.\n",
        "    \"\"\"\n",
        "    positions = {}\n",
        "    for entity_name, entity_value in entities.items():\n",
        "        start_pos = text.find(str(entity_value))\n",
        "        end_pos = start_pos + len(str(entity_value)) if start_pos != -1 else -1\n",
        "        positions[f\"{entity_name}_Start_Pos\"] = start_pos\n",
        "        positions[f\"{entity_name}_End_Pos\"] = end_pos\n",
        "    return positions\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_excel('labelled_dataset.xlsx')\n",
        "\n",
        "# Iterate over each row and compute the start and end positions for each entity\n",
        "for _, row in data.iterrows():\n",
        "    entity_dict = {\n",
        "        \"Company_Receiving_Investment\": row[\"Company_Receiving_Investment\"],\n",
        "        \"Investment_Amount\": row[\"Investment_Amount\"],\n",
        "        \"Investing_Companies\": row[\"Investing_Companies\"],\n",
        "        \"Round_of_Investment\": row[\"Round_of_Investment\"],\n",
        "        \"Location_of_the_Company\": row[\"Location_of_the_Company\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"CEO_Name\": row[\"CEO_Name\"]\n",
        "    }\n",
        "    positions = get_entity_positions(row[\"Article_Text\"], entity_dict)\n",
        "    for col, pos in positions.items():\n",
        "        data.at[_, col] = pos\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "data.to_excel(\"updated_labelled_dataset.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "LqSEPp_iA8xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOaZQp86HtH5",
        "outputId": "1fb17196-9352-4d2a-e7b6-b4dee06c9839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m804.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin, Span\n",
        "from spacy.training import Example\n",
        "import pandas as pd\n",
        "import random\n",
        "from spacy.util import minibatch  # Import minibatch utility\n",
        "\n",
        "# Load a better pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_lg\")  # Use the medium model for better performance\n",
        "\n",
        "# Create a DocBin object to store training examples\n",
        "doc_bin = DocBin()\n",
        "\n",
        "# Load your updated dataset\n",
        "data = pd.read_excel('updated_labelled_dataset.xlsx')\n",
        "\n",
        "# Iterate over the rows of the DataFrame to create training examples\n",
        "for _, row in data.iterrows():\n",
        "    text = row['Article_Text']\n",
        "    doc = nlp.make_doc(text)  # Create the Doc object\n",
        "\n",
        "    spans = []  # List to store spans\n",
        "    # Create spans for each entity based on start and end positions\n",
        "    for col in data.columns:\n",
        "        if 'Start_Pos' in col:  # Check for start position columns\n",
        "            start_col = col\n",
        "            end_col = start_col.replace('Start_Pos', 'End_Pos')\n",
        "            start = row[start_col]\n",
        "            end = row[end_col]\n",
        "            if start != -1 and end != -1 and end <= len(doc):  # Check for valid positions\n",
        "                label = start_col.replace('_Start_Pos', '')\n",
        "                if label in nlp.get_pipe(\"ner\").labels:  # Ensure the label is in the NER model\n",
        "                    spans.append(Span(doc, start, end, label=label))  # Create Span\n",
        "\n",
        "    # Create a list of entity tuples for the Example\n",
        "    entity_tuples = [(span.start, span.end, span.label_) for span in spans]\n",
        "\n",
        "    # Create an example for training\n",
        "    example = Example.from_dict(doc, {\"entities\": entity_tuples})\n",
        "\n",
        "    # Add the doc to the DocBin\n",
        "    doc_bin.add(doc)  # Add the actual Doc object, not the Example\n",
        "\n",
        "# Save the training data to disk\n",
        "doc_bin.to_disk(\"training_data.spacy\")\n",
        "\n",
        "# **Training the Model**\n",
        "# Check if the NER pipeline exists; if not, create one\n",
        "if \"ner\" not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe(\"ner\")\n",
        "    nlp.add_pipe(ner, last=True)\n",
        "\n",
        "# Add your new labels to the NER model\n",
        "for label in data.columns:\n",
        "    if 'Start_Pos' not in label and 'End_Pos' not in label and label != 'Article_Text':\n",
        "        ner.add_label(label)\n",
        "\n",
        "# Prepare for training\n",
        "optimizer = nlp.resume_training()\n",
        "random.seed(42)  # For reproducibility\n",
        "losses = {}  # Dictionary to store losses\n",
        "\n",
        "# Training loop\n",
        "for iteration in range(10):  # Adjust number of iterations as necessary\n",
        "    docs = list(doc_bin.get_docs(nlp.vocab))  # Convert generator to list\n",
        "    random.shuffle(docs)  # Shuffle the training data\n",
        "\n",
        "    # Create minibatches\n",
        "    batches = minibatch(docs, size=8)  # Size can be adjusted\n",
        "    for batch in batches:\n",
        "        examples = []\n",
        "        for doc in batch:\n",
        "            # Extracting spans for the current document\n",
        "            text = doc.text\n",
        "            spans = []\n",
        "            # Find the corresponding row in your dataset for this document\n",
        "            row = data[data['Article_Text'] == text].iloc[0]\n",
        "            for col in data.columns:\n",
        "                if 'Start_Pos' in col:  # Check for start position columns\n",
        "                    start_col = col\n",
        "                    end_col = start_col.replace('Start_Pos', 'End_Pos')\n",
        "                    start = row[start_col]\n",
        "                    end = row[end_col]\n",
        "                    if start != -1 and end != -1 and end <= len(doc):  # Check for valid positions\n",
        "                        label = start_col.replace('_Start_Pos', '')\n",
        "                        if label in nlp.get_pipe(\"ner\").labels:  # Ensure the label is in the NER model\n",
        "                            spans.append(Span(doc, start, end, label=label))  # Create Span\n",
        "\n",
        "            if spans:\n",
        "                # Create example with extracted spans\n",
        "                example = Example.from_dict(doc, {\"entities\": [(span.start, span.end, span.label_) for span in spans]})\n",
        "                examples.append(example)\n",
        "\n",
        "        # Update the model\n",
        "        nlp.update(examples, drop=0.5, losses=losses)\n",
        "\n",
        "# Save the trained model\n",
        "nlp.to_disk(\"trained_ner_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe7LP1j0DRLf",
        "outputId": "47446aaf-702b-448d-da0a-f74e040dda5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the trained model\n",
        "nlp = spacy.load(\"trained_ner_model\")\n",
        "\n",
        "# Sample text for testing\n",
        "test_text = \"\"\"With Nothing Underneath, a London, UK-based womenswear brand, raised £2.5M in funding.\n",
        "The round was led by Pembroke VCT alongside JamJar Investments.\n",
        "The company intends to use the funds to accelerate the expansion of its team and of its UK retail presence,\n",
        "grow its presence in overseas markets in the U.S. and Europe – specifically in the DACH region (Germany, Austria and Switzerland),\n",
        "and further develop new product lines. Founded in 2017 by Pip Durell, former Tatler editor and Vogue stylist,\n",
        "With Nothing Underneath provides shirts and other wardrobe staples with inspiration drawn from icons like Carolyn Bessette and Charlotte Rampling.\n",
        "Their sourcing and practices are recognised by a B Corp certification. The business has already generated an international customer base\n",
        "and received organic endorsements from the likes of Dolly Alderton, Cara Delevingne and Florence Pugh.\n",
        "It has also collaborated with well-known brand, Free People, and has a limited-edition partnership with the Financial Times.\n",
        "FinSMEs 21/10/2024\"\"\"\n",
        "\n",
        "# Process the text with the NER model\n",
        "doc = nlp(test_text)\n",
        "\n",
        "# Print the recognized entities\n",
        "print(\"Entities found:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} - {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwzUH6pDFNra",
        "outputId": "9f4bdb35-0afc-46bf-cf41-09004d36866e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities found:\n",
            "London - GPE\n",
            "UK - GPE\n",
            "£2.5M - MONEY\n",
            "Pembroke VCT - ORG\n",
            "JamJar Investments - ORG\n",
            "UK - GPE\n",
            "U.S. - GPE\n",
            "Europe - LOC\n",
            "DACH - GPE\n",
            "Germany - GPE\n",
            "Austria - GPE\n",
            "Switzerland - GPE\n",
            "2017 - DATE\n",
            "Pip Durell - PERSON\n",
            "Tatler - ORG\n",
            "Vogue - ORG\n",
            "Carolyn Bessette - PERSON\n",
            "Charlotte Rampling - PERSON\n",
            "B Corp - ORG\n",
            "Dolly Alderton - PERSON\n",
            "Cara Delevingne - PERSON\n",
            "Florence Pugh - PERSON\n",
            "Free People - ORG\n",
            "the Financial Times - ORG\n",
            "21/10/2024 - DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load the SpaCy model (ensure it's the correct model for your needs)\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Or use a larger model if needed\n",
        "\n",
        "# Sample text from the article about Elixirr and Hypothesis\n",
        "text = \"\"\"\n",
        "Elixirr, a London, UK-based consulting firm, acquired Hypothesis, a Los Angeles, CA-based insights and strategy firm.\n",
        "The amount of the deal was not disclosed. The acquisition brings new capabilities to Elixirr’s client base.\n",
        "Hypothesis’ roster of clients, with a particular focus on the consumer, technology and entertainment industries,\n",
        "is highly complementary to Elixirr’s established strategy, digital, data, AI and innovation offering.\n",
        "Led by CEO Maria Vallis, Hypothesis is an insights and strategy agency for brands to innovate, uncover opportunities, and grow.\n",
        "They help clients understand audiences and attract new customers, position, build, and strengthen brands,\n",
        "create content and communications, develop products and services that deliver growth,\n",
        "and operate with purpose and create values-driven culture. Through this acquisition, Vallis will now join Elixirr as Partner,\n",
        "based out of Los Angeles, California. Founded in 2009, and led by CEO Stephen Newton, Elixirr is a global consulting firm\n",
        "working with clients across a diverse range of industries, markets and geographies.\n",
        "It has also acquired seven boutique firms – Den Creative, Coast Digital, The Retearn Group, iOLAP, Responsum, Insigniam\n",
        "and now Hypothesis – to grow capabilities, expand into new geographies and markets, access new clients and talent, and more.\n",
        "Elixirr has been quoted on the AIM market of the London Stock Exchange since 2020. FinSMEs 21/10/2024\n",
        "\"\"\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Initialize the data structure for storing classified entities\n",
        "classified_entities = {\n",
        "    \"Company_Receiving_Investment\": None,\n",
        "    \"Investment_Amount\": None,\n",
        "    \"Investing_Companies\": [],\n",
        "    \"Round_of_Investment\": None,\n",
        "    \"Location_of_the_Company\": [],\n",
        "    \"Date\": None,\n",
        "    \"CEO_Name\": None,\n",
        "}\n",
        "\n",
        "# Iterate over the detected entities and classify them\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ == \"ORG\":\n",
        "        if \"VCT\" in ent.text or \"Investments\" in ent.text:\n",
        "            classified_entities[\"Investing_Companies\"].append(ent.text)\n",
        "        else:\n",
        "            classified_entities[\"Company_Receiving_Investment\"] = ent.text\n",
        "    elif ent.label_ == \"MONEY\":\n",
        "        classified_entities[\"Investment_Amount\"] = ent.text\n",
        "    elif ent.label_ == \"GPE\":\n",
        "        classified_entities[\"Location_of_the_Company\"].append(ent.text)\n",
        "    elif ent.label_ == \"DATE\":\n",
        "        classified_entities[\"Date\"] = ent.text\n",
        "    elif ent.label_ == \"PERSON\":\n",
        "        if not classified_entities[\"CEO_Name\"]:  # Store only the first name as CEO\n",
        "            classified_entities[\"CEO_Name\"] = ent.text\n",
        "\n",
        "# Print the structured data\n",
        "print(classified_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49jOhLHGFUIx",
        "outputId": "2acdb183-d0a3-45db-df4a-520074ddff22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Company_Receiving_Investment': 'the London Stock Exchange', 'Investment_Amount': None, 'Investing_Companies': [], 'Round_of_Investment': None, 'Location_of_the_Company': ['London', 'UK', 'Los Angeles', 'Los Angeles', 'California'], 'Date': '21/10/2024', 'CEO_Name': 'Maria Vallis'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your new labels to the NER model\n",
        "for label in data.columns:\n",
        "    if 'Start_Pos' not in label and 'End_Pos' not in label and label != 'Article_Text':\n",
        "        ner.add_label(label)  # Ensure these labels are present in training\n"
      ],
      "metadata": {
        "id": "ZI7eEBBFIPf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Custom labels added to the NER model:\", [label for label in data.columns if 'Start_Pos' not in label and 'End_Pos' not in label and label != 'Article_Text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJA5EGGfIVj8",
        "outputId": "9e07931f-e1c2-4cb7-971a-42b9702c7468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom labels added to the NER model: ['Article_No', 'Company_Receiving_Investment', 'Investment_Amount', 'Investing_Companies', 'Round_of_Investment', 'Location_of_the_Company', 'Date', 'CEO_Name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range(10):  # Adjust number of iterations as necessary\n",
        "    docs = list(doc_bin.get_docs(nlp.vocab))  # Convert generator to list\n",
        "    random.shuffle(docs)  # Shuffle the training data\n",
        "\n",
        "    # Create minibatches\n",
        "    batches = minibatch(docs, size=8)  # Size can be adjusted\n",
        "    for batch in batches:\n",
        "        examples = []\n",
        "        for doc in batch:\n",
        "            spans = [...]  # Extract spans similarly to how you did during training\n",
        "            if spans:\n",
        "                example = Example.from_dict(doc, {\"entities\": [(span.start, span.end, span.label_) for span in spans]})\n",
        "                examples.append(example)\n",
        "                # Debugging: Print examples\n",
        "                print(f\"Training Example: {example}\")\n",
        "\n",
        "        # Update the model\n",
        "        nlp.update(examples, drop=0.5, losses=losses)\n",
        "\n",
        "print(\"Losses during training:\", losses)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "BF66CqeaIYBm",
        "outputId": "883b241b-03d2-4de0-c405-b0fef3881106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ellipsis' object has no attribute 'start'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-83a4724139a1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mspans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Extract spans similarly to how you did during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;31m# Debugging: Print examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-83a4724139a1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mspans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Extract spans similarly to how you did during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;31m# Debugging: Print examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'start'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "VXjzQJCbIfeO",
        "outputId": "5deeacd4-9b0e-43fb-9786-3d3ecdd946f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-05cf52d0a56e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxBa9r9ionu-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}